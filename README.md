

```md
# ğŸ£ Fish-Segmentation-DINOv3

This repository contains a complete semantic segmentation pipeline using **Meta AIâ€™s DINOv3 Vision Transformer** on the **â€œA Large-Scale Fish Datasetâ€** from Kaggle.  
The project demonstrates how a frozen self-supervised backbone combined with a lightweight decoder can achieve **95%+ IoU** on a real-world dataset containing diverse fish species and challenging imaging conditions.

---

# ğŸŸ Dataset

This project uses the **A Large-Scale Fish Dataset** from Kaggle:  
https://www.kaggle.com/datasets/crowww/a-large-scale-fish-dataset

### Dataset Properties
- ~9,000 labeled fish images  
- Pixel-accurate segmentation masks (`ClassName GT`)  
- Large variations in:
  - species  
  - fish shape/thickness  
  - rotation orientation  
  - lighting and glare  
  - background plates  
  - color variations  

These characteristics make the dataset ideal for evaluating general-purpose segmentation models.

---

# ğŸ¤– Why DINOv3 Works Extremely Well

Although DINOv3 is **self-supervised** and not explicitly trained for segmentation, it performs exceptionally due to:

### ğŸ”¹ 1. Strong global feature representation  
Vision Transformers capture long-range dependencies â†’ ideal for elongated fish bodies.

### ğŸ”¹ 2. High semantic separation  
The modelâ€™s learned representations naturally separate foreground (fish) from background.

### ğŸ”¹ 3. Patch-level structure preservation  
ViT patch embeddings preserve body contours, improving mask sharpness.

### ğŸ”¹ 4. Frozen backbone â†’ stable training  
Only a small CNN decoder is trained.  
No risk of overfitting.  
Fast, consistent convergence.

---

# âš™ï¸ Model Architecture

```

Input Image
â†“
DINOv3 ViT Backbone (Frozen)
â†“ patch embeddings
Reshaped into a 2D grid (H/16 Ã— W/16)
â†“
Lightweight 3-layer CNN Decoder
â†“
Upsampled segmentation mask (1 Ã— H Ã— W)

````

---

# ğŸ“ˆ Training Configuration

- Backbone: **facebook/dinov3-vits16-pretrain-lvd1689m**  
- Decoder: **3-layer CNN**  
- Loss: **BCEWithLogitsLoss**  
- Optimizer: **AdamW**  
- Image size: **448Ã—448**  
- Epochs: **20**  
- Batch size: **8**  
- Metric: **IoU (Intersection-over-Union)**  

---

# ğŸ“Š Training Curves

## ğŸ”¹ Train Iteration Loss
![Train Iteration Loss](plots/1_Train_Iteration_Loss.png)

## ğŸ”¹ Train Iteration IoU
![Train Iteration IoU](plots/2_Train_Iteration_IoU.png)

## ğŸ”¹ Train vs Validation Loss (Mean Â± Std)
![Train vs Validation Loss](plots/3_Train_Val_Loss_MeanStd.png)

## ğŸ”¹ Validation Iteration Loss
![Val Iteration Loss](plots/4_Val_Iteration_Loss.png)

## ğŸ”¹ Train vs Validation IoU (Mean Â± Std)
![Train vs Validation IoU](plots/5_Train_Val_IoU_MeanStd.png)

## ğŸ”¹ Validation Iteration IoU
![Val Iteration IoU](plots/6_Val_Iteration_IoU.png)

---

# ğŸ“œ Epoch-by-Epoch IoU Log (Screenshot)

This screenshot shows the steady increase in IoU over epochs.

![Epoch Log](plots/EkranResmi.png)

*(Rename your file to `EkranResmi.png` inside `plots/` before pushing.)*

---

# ğŸ² Qualitative Results

## ğŸ”¹ 20 Random Predictions
Demonstrates strong generalization across species, lighting, and rotation.

![Random Predictions](plots/Unknown-8.png)

## ğŸ¥‡ Best 5 Predictions (IoU ~98.3â€“98.8%)
Nearly perfect overlaps between predicted and ground truth masks.

![Best Predictions](plots/Unknown-10.png)

## âš ï¸ Worst 5 Predictions (IoU ~56â€“79%)
Performance drops for:
- extreme rotations  
- very thin fish  
- heavy reflections  
- occlusions  

![Worst Predictions](plots/Unknown-9.png)

---

# ğŸš€ Installation

```bash
git clone https://github.com/PEPEZHK/Fish-Segmentation-DINOv3
cd Fish-Segmentation-DINOv3
pip install -r requirements.txt
````

---

# ğŸ‹ï¸â€â™‚ï¸ Training

```bash
python src/train.py \
    --data-root "/path/to/fish_dataset/Fish_Dataset/Fish_Dataset" \
    --epochs 20 \
    --batch-size 8 \
    --image-size 448
```

---

# ğŸ” Evaluation Only

```bash
python src/train.py --eval-only
```

---

# ğŸ“¦ Repository Structure

```
Fish-Segmentation-DINOv3/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ DINOv3_Fish_Segmentation_Final.ipynb
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ train.py
â”‚
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ 1_Train_Iteration_Loss.png
â”‚   â”œâ”€â”€ 2_Train_Iteration_IoU.png
â”‚   â”œâ”€â”€ 3_Train_Val_Loss_MeanStd.png
â”‚   â”œâ”€â”€ 4_Val_Iteration_Loss.png
â”‚   â”œâ”€â”€ 5_Train_Val_IoU_MeanStd.png
â”‚   â”œâ”€â”€ 6_Val_Iteration_IoU.png
â”‚   â”œâ”€â”€ Unknown-8.png
â”‚   â”œâ”€â”€ Unknown-9.png
â”‚   â”œâ”€â”€ Unknown-10.png
â”‚   â””â”€â”€ EkranResmi.png
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ checkpoints/
â”‚
â””â”€â”€ README.md
```

---

# ğŸ Conclusion

This project shows how a **frozen DINOv3 backbone + small decoder** can deliver high-quality segmentation masks with:

* **95%+ IoU**
* fast training
* strong robustness to rotation and lighting
* minimal overfitting

DINOv3 proves to be a powerful universal feature extractor for downstream vision tasks.

---

```

---

# âœ… Done â€” This is the *complete* README in one block.  
If you want:

ğŸ“Œ badges  
ğŸ“Œ inference script section  
ğŸ“Œ architecture diagram  
ğŸ“Œ citation section  

â€”I can add them too.
```
